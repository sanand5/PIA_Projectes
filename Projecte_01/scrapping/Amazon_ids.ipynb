{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obteniendo productos de la página 1...\n",
      "Obteniendo productos de la página 2...\n",
      "Obteniendo productos de la página 3...\n",
      "Obteniendo productos de la página 4...\n",
      "Obteniendo productos de la página 5...\n",
      "Obteniendo productos de la página 6...\n",
      "Obteniendo productos de la página 7...\n",
      "Obteniendo productos de la página 8...\n",
      "Obteniendo productos de la página 9...\n",
      "Obteniendo productos de la página 10...\n",
      "Obteniendo productos de la página 11...\n",
      "Obteniendo productos de la página 12...\n",
      "Obteniendo productos de la página 13...\n",
      "No se encontraron productos en la página 13. Terminando la recolección.\n",
      "Se guardaron 503 IDs de productos en 'ids.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Constantes\n",
    "BASE_URL = \"https://www.amazon.es/s?k=\"\n",
    "\n",
    "# Función para obtener el contenido HTML de una página usando ScraperAPI\n",
    "def obtener_soup(api_key, url):\n",
    "    params = {\n",
    "        'api_key': api_key,\n",
    "        'url': url,\n",
    "        'country_code': 'ES'  # Código de país para España\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://api.scraperapi.com/', params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    else:\n",
    "        print(f'Error al obtener la página: {response.status_code}')\n",
    "        return None\n",
    "\n",
    "# Función para obtener los links de productos en una página\n",
    "def obtener_links_productos(soup):\n",
    "    links = []\n",
    "    productos = soup.find_all('a', {'class': 'a-link-normal'})\n",
    "    for producto in productos:\n",
    "        href = producto.get('href')\n",
    "        if href and '/dp/' in href:\n",
    "            links.append(\"https://www.amazon.es\" + href)\n",
    "    return links\n",
    "\n",
    "# Función para obtener los IDs de los productos a partir de los links\n",
    "def obtener_ids_productos(links):\n",
    "    ids_productos = set()  # Usamos un set para evitar duplicados\n",
    "    for link in links:\n",
    "        product_id = link.split('/dp/')[1].split('/')[0]\n",
    "        ids_productos.add(product_id)  # Agregar al set, los duplicados se eliminan automáticamente\n",
    "    return ids_productos\n",
    "\n",
    "# Función principal para recolectar los IDs de productos\n",
    "def recolectar_ids(api_key, query):\n",
    "    url_base = BASE_URL + query\n",
    "    page_number = 1\n",
    "    ids_productos = set()  # Usamos un set para evitar duplicados\n",
    "\n",
    "    while True:\n",
    "        print(f\"Obteniendo productos de la página {page_number}...\")\n",
    "        url_pagina = f\"{url_base}&page={page_number}\"\n",
    "        soup = obtener_soup(api_key, url_pagina)\n",
    "        \n",
    "        if not soup:\n",
    "            print(f\"No se pudo obtener la página {page_number}. Terminando la recolección.\")\n",
    "            break\n",
    "\n",
    "        # Obtener los links de productos de la página\n",
    "        links_productos = obtener_links_productos(soup)\n",
    "        if not links_productos:\n",
    "            print(f\"No se encontraron productos en la página {page_number}. Terminando la recolección.\")\n",
    "            break\n",
    "\n",
    "        # Obtener los IDs de los productos y agregar al set\n",
    "        ids_pagina = obtener_ids_productos(links_productos)\n",
    "        ids_productos.update(ids_pagina)  # Usamos .update() para agregar sin duplicados\n",
    "\n",
    "        # Aumentar el número de página\n",
    "        page_number += 1\n",
    "\n",
    "    # Filtrar los IDs para eliminar los que tengan más de 10 caracteres\n",
    "    ids_productos_filtrados = [id for id in ids_productos if len(id) <= 10]\n",
    "\n",
    "    # Guardar los IDs filtrados en un archivo JSON\n",
    "    with open(\"ids.json\", \"w\") as f:\n",
    "        json.dump(ids_productos_filtrados, f, indent=4)\n",
    "\n",
    "    print(f\"Se guardaron {len(ids_productos_filtrados)} IDs de productos en 'ids.json'.\")\n",
    "\n",
    "# Ejecución del programa\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = '83bc7b5f85e1d83b695e0d39816ebb77'  # Tu API Key\n",
    "    query = \"moviles\"  # Tu consulta de búsqueda\n",
    "    \n",
    "    recolectar_ids(API_KEY, query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porque va amas paguinas de las que existen?\n",
    "Porque hay que tienen mayor de 10?\n",
    "porque hay repetidos?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projecte01_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
