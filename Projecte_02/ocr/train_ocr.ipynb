{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Configuración básica\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "chars = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz \"\n",
    "num_chars = len(chars) + 1  # +1 para blank token de CTC\n",
    "img_size = (64, 256)  # Alto, Ancho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "        \n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for img_name in os.listdir(self.img_dir):\n",
    "            if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(self.img_dir, img_name)\n",
    "                label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "                if os.path.exists(label_path):\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        for line in f.readlines():\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 5:\n",
    "                                text = ' '.join(parts[4:])\n",
    "                                bbox = list(map(float, parts[:4]))\n",
    "                                samples.append((img_path, bbox, text))\n",
    "        return samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, bbox, text = self.samples[idx]\n",
    "        \n",
    "        # Leer y procesar imagen\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        h, w = img.shape\n",
    "        \n",
    "        # Convertir bbox YOLO a coordenadas absolutas\n",
    "        x_center, y_center, bw, bh = bbox\n",
    "        x1 = int((x_center - bw/2) * w)\n",
    "        y1 = int((y_center - bh/2) * h)\n",
    "        x2 = int((x_center + bw/2) * w)\n",
    "        y2 = int((y_center + bh/2) * h)\n",
    "        \n",
    "        # Recortar región de texto\n",
    "        roi = img[max(0,y1):min(h,y2), max(0,x1):min(w,x2)]\n",
    "        roi = cv2.resize(roi, (img_size[1], img_size[0]))\n",
    "        roi = Image.fromarray(roi)\n",
    "        \n",
    "        if self.transform:\n",
    "            roi = self.transform(roi)\n",
    "        \n",
    "        # Convertir texto a índices\n",
    "        target = [chars.index(c) for c in text if c in chars]\n",
    "        return roi, torch.tensor(target, dtype=torch.long), torch.tensor(len(target), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Función para hacer padding de los batches\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    lengths = [item[2] for item in batch]\n",
    "    \n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    # Padding de targets\n",
    "    max_len = max([len(t) for t in targets])\n",
    "    padded_targets = torch.zeros(len(targets), max_len, dtype=torch.long)\n",
    "    for i, target in enumerate(targets):\n",
    "        padded_targets[i, :len(target)] = target\n",
    "    \n",
    "    lengths = torch.stack(lengths, 0)\n",
    "    return images, padded_targets, lengths\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "train_dataset = OCRDataset(\"dataset2/images/train\", \"dataset2/labels/train\", transform)\n",
    "val_dataset = OCRDataset(\"dataset2/images/val\", \"dataset2/labels/val\", transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # CNN\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            nn.Conv2d(512, 512, 2), nn.BatchNorm2d(512), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.LSTM(512, 128, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        \n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(256, num_chars + 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        conv = self.cnn(x)  # [batch, channels, height, width]\n",
    "        \n",
    "        # Preparar para RNN\n",
    "        conv = conv.squeeze(2)  # Eliminar dimensión de altura [batch, channels, width]\n",
    "        conv = conv.permute(2, 0, 1)  # [width, batch, channels]\n",
    "        \n",
    "        # RNN\n",
    "        rnn_out, _ = self.rnn(conv)\n",
    "        \n",
    "        # Salida\n",
    "        output = self.fc(rnn_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, targets, target_lengths) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        input_lengths = torch.full((outputs.size(1),), outputs.size(0), dtype=torch.long).to(device)\n",
    "        \n",
    "        loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(dataloader)} - Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, target_lengths in dataloader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            input_lengths = torch.full((outputs.size(1),), outputs.size(0), dtype=torch.long).to(device)\n",
    "            \n",
    "            loss = criterion(outputs, targets, input_lengths, target_lengths)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 9\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m target_lengths \u001b[38;5;241m=\u001b[39m target_lengths\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m input_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m),), outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets, input_lengths, target_lengths)\n",
      "File \u001b[0;32m~/Escritorio/VISUAL_STUDIO/gerard/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Escritorio/VISUAL_STUDIO/gerard/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[67], line 31\u001b[0m, in \u001b[0;36mCRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Preparar para RNN\u001b[39;00m\n\u001b[1;32m     30\u001b[0m conv \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Eliminar dimensión de altura [batch, channels, width]\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m conv \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [width, batch, channels]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# RNN\u001b[39;00m\n\u001b[1;32m     34\u001b[0m rnn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(conv)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "# Inicialización\n",
    "model = CRNN(len(chars)).to(device)\n",
    "criterion = nn.CTCLoss(blank=len(chars))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}')\n",
    "    \n",
    "    # Guardar el mejor modelo\n",
    "    if epoch == 0 or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_ocr_model.pth')\n",
    "        print('Modelo guardado!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, transform, device):\n",
    "    # Preprocesamiento\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (img_size[1], img_size[0]))\n",
    "    img = Image.fromarray(img)\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predicción\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, preds = torch.max(outputs, 2)\n",
    "        pred_text = decode_ctc(preds.squeeze(), chars)\n",
    "    \n",
    "    return pred_text\n",
    "\n",
    "def decode_ctc(sequence, chars):\n",
    "    prev_char = None\n",
    "    text = []\n",
    "    for idx in sequence:\n",
    "        if idx < len(chars) and (prev_char != idx or idx == len(chars)):\n",
    "            if idx == len(chars):  # Blank token\n",
    "                prev_char = None\n",
    "            else:\n",
    "                text.append(chars[idx])\n",
    "                prev_char = idx\n",
    "    return ''.join(text)\n",
    "\n",
    "# Ejemplo de uso\n",
    "# model.load_state_dict(torch.load('best_ocr_model.pth'))\n",
    "# text = predict(\"ejemplo.jpg\", model, transform, device)\n",
    "# print(\"Texto reconocido:\", text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gerard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
